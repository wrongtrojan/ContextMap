import os
import json
import yaml
import logging
from pathlib import Path
from minio import Minio
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility

# ================= æ—¥å¿—é…ç½® =================
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - [UnifiedIngestor] - %(levelname)s - %(message)s'
)
logger = logging.getLogger("MilvusIngestor")

class UnifiedIngestor:
    def __init__(self, force_reset=False):
        # 1. è·¯å¾„ä¸é…ç½®åŠ è½½
        self.project_root = Path(__file__).resolve().parent.parent
        config_dir = self.project_root / "configs"
        
        with open(config_dir / "model_config.yaml", 'r', encoding='utf-8') as f:
            self.model_cfg = yaml.safe_load(f)
        with open(config_dir / "milvus_config.yaml", 'r', encoding='utf-8') as f:
            self.db_cfg = yaml.safe_load(f)

        # 2. åˆå§‹åŒ– MinIO å®¢æˆ·ç«¯ (åŸºäºä½ çš„ docker ps è¾“å‡º)
        # æ³¨æ„ï¼šè¿™é‡Œå»ºè®®åœ¨ milvus_config.yaml ä¸­å¢åŠ  minio é…ç½®ï¼Œç›®å‰å…ˆç¡¬ç¼–ç ç¡®ä¿èƒ½è·‘
        self.minio_client = Minio(
            "localhost:9000",
            access_key="minioadmin",  # è¯·ç¡®è®¤ä½ çš„ MinIO è´¦å·å¯†ç 
            secret_key="minioadmin",
            secure=False
        )
        self.bucket_name = "academic-assets"
        self._setup_minio()

        # 3. è¿æ¥ Milvus
        conn = self.db_cfg['connection']
        connections.connect("default", host=conn['host'], port=conn['port'])
        self.col_name = self.db_cfg['collection']['name']
        
        if force_reset and utility.has_collection(self.col_name):
            utility.drop_collection(self.col_name)
            logger.warning(f"âš ï¸ å·²å¼ºåˆ¶é‡ç½®é›†åˆ: {self.col_name}")

        self._setup_collection()

    def _setup_minio(self):
        """ç¡®ä¿ MinIO Bucket å­˜åœ¨å¹¶è®¾ç½®å…¬å…±åªè¯»æƒé™"""
        if not self.minio_client.bucket_exists(self.bucket_name):
            self.minio_client.make_bucket(self.bucket_name)
            # è®¾ç½®ç­–ç•¥è®© Attu èƒ½å¤Ÿç›´æ¥é¢„è§ˆå›¾ç‰‡ï¼ˆåŒ¿åå¯è¯»ï¼‰
            policy = {
                "Version": "2012-10-17",
                "Statement": [{
                    "Effect": "Allow",
                    "Principal": {"AWS": ["*"]},
                    "Action": ["s3:GetBucketLocation", "s3:ListBucket", "s3:GetObject"],
                    "Resource": [f"arn:aws:s3:::{self.bucket_name}", f"arn:aws:s3:::{self.bucket_name}/*"]
                }]
            }
            self.minio_client.set_bucket_policy(self.bucket_name, json.dumps(policy))
            logger.info(f"ğŸ“¦ MinIO Bucket '{self.bucket_name}' å·²åˆ›å»ºå¹¶é…ç½®æƒé™")

    def _upload_file(self, local_path, remote_path):
        # 1. è½¬æ¢è·¯å¾„å¯¹è±¡
        p = Path(local_path)
        
        # 2. å­˜åœ¨æ€§æ£€æŸ¥ (æ ¸å¿ƒä¿®å¤)
        if not p.exists():
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ : {p}")
            return None # è¿”å› Noneï¼Œåç»­å…¥åº“é€»è¾‘ä¼šè‡ªåŠ¨å¿½ç•¥æ­¤æ¡ç›®

        # 3. æ ¡éªŒæ˜¯å¦ä¸ºæ–‡ä»¶è€Œéç›®å½•
        if not p.is_file():
            logger.error(f"âš ï¸ è·¯å¾„ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶: {p}")
            return None

        # 4. 0 å­—èŠ‚æŸåæ£€æŸ¥
        if p.stat().st_size == 0:
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ° 0 å­—èŠ‚æŸåæ–‡ä»¶ï¼Œè·³è¿‡: {p}")
            return None

        try:
            # æ‰§è¡Œä¸Šä¼ 
            self.minio_client.fput_object(self.bucket_name, remote_path, str(p))
            
            # æ„é€ è®¿é—® URL (ä½¿ç”¨æœåŠ¡å™¨çœŸå® IP)
            server_ip = "202.114.104.220" # è¯·ç¡®ä¿è¿™æ˜¯ä½ æœ€æ–°çš„æœåŠ¡å™¨ IP
            return f"http://{server_ip}:9000/{self.bucket_name}/{remote_path}"
            
        except Exception as e:
            logger.error(f"ğŸ”¥ MinIO ä¸Šä¼ è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e} | æ–‡ä»¶: {p.name}")
            return None

    def _setup_collection(self):
        """å®šä¹‰ Schema å¹¶æ£€æŸ¥å…¼å®¹æ€§"""
        if utility.has_collection(self.col_name):
            col = Collection(self.col_name)
            # æ£€æŸ¥ asset_name å­—æ®µæ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™è§†ä¸ºæ—§ç‰ˆ
            if "asset_name" not in [f.name for f in col.schema.fields]:
                logger.warning("æ£€æµ‹åˆ°æ—§ç‰ˆç»“æ„ï¼Œæ­£åœ¨é‡å»º...")
                utility.drop_collection(self.col_name)
            else:
                self.collection = col
                self.collection.load()
                return

        c = self.db_cfg['collection']
        s = self.db_cfg['schema']
        fields = [
            FieldSchema(name=s['pk'], dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="asset_name", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="modality", dtype=DataType.VARCHAR, max_length=50),      
            FieldSchema(name="content_type", dtype=DataType.VARCHAR, max_length=50),  
            FieldSchema(name="content_ref", dtype=DataType.VARCHAR, max_length=1000), 
            FieldSchema(name="timestamp", dtype=DataType.DOUBLE),                    
            FieldSchema(name=s['vec'], dtype=DataType.FLOAT_VECTOR, dim=c['dim'])
        ]
        schema = CollectionSchema(fields, "Unified Academic Assets with MinIO URLs")
        self.collection = Collection(self.col_name, schema)
        
        index_params = {"metric_type": c['metric_type'], "index_type": c['index_type'], "params": {"nlist": c['nlist']}}
        self.collection.create_index(field_name=s['vec'], index_params=index_params)
        self.collection.load()
        logger.info(f"âœ… Unified Milvus Collection Loaded (with MinIO support)")

    def _is_ingested(self, asset_name, modality):
        expr = f'asset_name == "{asset_name}" and modality == "{modality}"'
        res = self.collection.query(expr=expr, output_fields=["id"], limit=1)
        return len(res) > 0

    def ingest_pdf_data(self):
        """PDF å…¥åº“ï¼šåŒæ­¥ä¸Šä¼ å›¾ç‰‡åˆ° MinIO"""
        pdf_root = Path(self.model_cfg['paths']['processed_storage']) / "magic-pdf"
        if not pdf_root.exists(): return

        for doc_dir in pdf_root.iterdir():
            if not doc_dir.is_dir(): continue
            feature_path = doc_dir / "multimodal_features.json"
            if not feature_path.exists() or self._is_ingested(doc_dir.name, "pdf"):
                logger.info(f"â­ï¸  [SKIP] PDF: {doc_dir.name}")
                continue

            with open(feature_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æ‰¾åˆ°å›¾ç‰‡ç›®å½• (ocr æˆ– auto)
            img_dir = None
            for sub in ["auto", "ocr"]:
                if (doc_dir / sub / "images").exists():
                    img_dir = doc_dir / sub / "images"
                    break

            names, modalities, types, refs, timestamps, vecs = [], [], [], [], [], []

            # 1. å¤„ç†å›¾ç‰‡ï¼šå…ˆä¸Šä¼  MinIOï¼Œå†è®°ä¸‹ URL
            for img_name, vec in data.get("images", {}).items():
                remote_url = img_name
                if img_dir:
                    remote_url = self._upload_file(img_dir / img_name, f"pdf/{doc_dir.name}/{img_name}")
                
                names.append(doc_dir.name); modalities.append("pdf")
                types.append("image"); refs.append(remote_url)
                timestamps.append(-1.0); vecs.append(vec)

            # 2. å¤„ç†æ–‡æœ¬å—
            for chunk in data.get("text_chunks", []):
                names.append(doc_dir.name); modalities.append("pdf")
                types.append("text"); refs.append(chunk.get("text_slice", ""))
                timestamps.append(-1.0); vecs.append(chunk["embedding"])

            if names:
                self.collection.insert([names, modalities, types, refs, timestamps, vecs])
                logger.info(f"âœ… [DONE] PDF {doc_dir.name} å…¥åº“å®Œæˆ (å«å›¾ç‰‡ä¸Šä¼ )")

    def ingest_video_data(self):
        """è§†é¢‘å…¥åº“ï¼šåŒæ­¥ä¸Šä¼ å…³é”®å¸§åˆ° MinIO"""
        video_root = Path(self.model_cfg['paths']['processed_storage']) / "video"
        if not video_root.exists(): return

        for v_dir in video_root.iterdir():
            if not v_dir.is_dir(): continue
            meta_path = v_dir / "alignment_metadata.json"
            
            # å¢é‡æ£€æŸ¥
            if not meta_path.exists() or self._is_ingested(v_dir.name, "video"):
                logger.info(f"â­ï¸  [SKIP] Video: {v_dir.name}")
                continue

            with open(meta_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # å‡†å¤‡å…­ä¸ªå¯¹é½çš„åˆ—è¡¨
            names, modalities, types, refs, timestamps, vecs = [], [], [], [], [], []
            frames_dir = v_dir / "frames"

            for item in data.get("alignments", []):
                # --- 1. å¤„ç†å›¾åƒå¸§ (éœ€è¿‡ MinIO) ---
                img_path = frames_dir / item['frame_name']
                remote_url = self._upload_file(img_path, f"video/{v_dir.name}/{item['frame_name']}")
                
                # åªæœ‰ä¸Šä¼ æˆåŠŸä¸”æœ‰å‘é‡æ—¶æ‰å…¥åº“
                if remote_url and item.get("img_vector"):
                    names.append(v_dir.name)
                    modalities.append("video")
                    types.append("image_frame")
                    refs.append(remote_url)
                    timestamps.append(item['timestamp'])
                    vecs.append(item['img_vector'])

                # --- 2. å¤„ç†å…³è”æ–‡æœ¬ (ç›´æ¥å…¥åº“) ---
                if item.get("text_vector"):
                    names.append(v_dir.name)
                    modalities.append("video")
                    types.append("transcript_context")
                    # å­˜å…¥æ–‡æœ¬å†…å®¹å‰ 500 å­—ä½œä¸ºå‚è€ƒ
                    refs.append(item.get('context_text', '')[:500])
                    timestamps.append(item['timestamp'])
                    vecs.append(item['text_vector'])

            # æ‰¹é‡æ’å…¥
            if names:
                self.collection.insert([names, modalities, types, refs, timestamps, vecs])
                logger.info(f"âœ… [DONE] è§†é¢‘ {v_dir.name} å…¥åº“å®Œæˆ (å…± {len(names)} æ¡å‘é‡è®°å½•)")

    def finish(self):
        self.collection.flush()
        logger.info("âœ¨ æ•°æ®åŒæ­¥ä¸ MinIO æ˜ å°„åœ†æ»¡å®Œæˆ")

if __name__ == "__main__":
    # ç¬¬ä¸€æ¬¡è¿è¡Œå»ºè®® force_reset=True ä»¥åº”ç”¨åŒ…å« content_ref(URL) çš„é€»è¾‘
    ingestor = UnifiedIngestor(force_reset=False)
    ingestor.ingest_pdf_data()
    ingestor.ingest_video_data()
    ingestor.finish()